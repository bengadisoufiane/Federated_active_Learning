{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Importing Libraries"
      ],
      "metadata": {
        "id": "YkowUJDm3L5K"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "# import libraries\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.layers import Dense, Input\n",
        "from tensorflow.keras import Sequential\n",
        "from tensorflow.math import confusion_matrix\n",
        "from matplotlib.ticker import MultipleLocator\n",
        "import random\n",
        "from json import dump, load\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from keras.utils.np_utils import to_categorical\n",
        "import numpy as np \n",
        "import pandas as pd\n",
        "import os, re, time, math, tqdm, itertools\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import plotly.express as px\n",
        "import plotly.offline as pyo\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "import keras\n",
        "from keras.layers import Conv2D, Conv1D, MaxPooling2D, MaxPooling1D, Flatten, BatchNormalization, Dense\n",
        "from keras.utils.np_utils import to_categorical\n",
        "from keras.models import Sequential\n",
        "from keras.callbacks import CSVLogger, ModelCheckpoint\n",
        "plt.rcParams.update({'font.family':'Nimbus Roman'})"
      ],
      "outputs": [],
      "metadata": {
        "id": "w1EMB3LY3L5M"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Initializing Constants"
      ],
      "metadata": {
        "id": "WpHPrEf03L5M"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "NUM_ROUNDS = 15\n",
        "CLIENT_RATIO = 0.3\n",
        "TOTAL_CLIENTS = 100\n",
        "NUM_CLIENTS = int(CLIENT_RATIO * TOTAL_CLIENTS)\n",
        "INIT_THRESHOLD = 4.0\n",
        "EXPECTED_RESPONSE = 0.8"
      ],
      "outputs": [],
      "metadata": {
        "id": "Q4nhY96h3L5N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Importing the Dataset"
      ],
      "metadata": {
        "id": "sjc4jAEZ3L5N"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "network_data = pd.read_csv('/content/02-14-2018.csv')\n"
      ],
      "outputs": [],
      "metadata": {
        "id": "mblEccB23L5N"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# drop null or missing columns\n",
        "cleaned_data = network_data.dropna()\n",
        "cleaned_data.isna().sum().to_numpy()\n",
        "# encode the column labels\n",
        "label_encoder = LabelEncoder()\n",
        "cleaned_data['Label']= label_encoder.fit_transform(cleaned_data['Label'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GiHMCM8YH2Vo",
        "outputId": "75b2fe4b-00c7-4880-f596-33d513b6bfcf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:6: SettingWithCopyWarning:\n",
            "\n",
            "\n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# make 3 seperate datasets for 3 feature labels\n",
        "data_1 = cleaned_data[cleaned_data['Label'] == 0]\n",
        "data_2 = cleaned_data[cleaned_data['Label'] == 1]\n",
        "data_3 = cleaned_data[cleaned_data['Label'] == 2]\n",
        "\n",
        "# make benign feature\n",
        "y_1 = np.zeros(data_1.shape[0])\n",
        "y_benign = pd.DataFrame(y_1)\n",
        "\n",
        "# make bruteforce feature\n",
        "y_2 = np.ones(data_2.shape[0])\n",
        "y_bf = pd.DataFrame(y_2)\n",
        "\n",
        "# make bruteforceSSH feature\n",
        "y_3 = np.full(data_3.shape[0], 2)\n",
        "y_ssh = pd.DataFrame(y_3)\n",
        "\n",
        "# merging the original dataframe\n",
        "X = pd.concat([data_1, data_2, data_3], sort=True)\n",
        "y = pd.concat([y_benign, y_bf, y_ssh], sort=True)\n",
        "train_dataset = pd.concat([data_1, data_2, data_3])\n"
      ],
      "metadata": {
        "id": "cgii1r1dH22M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# L labelled dataset, U unlabelled dataset"
      ],
      "metadata": {
        "id": "6_Rz61_NTyX2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "test_dataset = train_dataset.sample(frac=0.1)\n",
        "train_dataset, U_unlabelled_dataset = train_test_split(train_dataset, test_size=0.8)\n",
        "\n",
        "target_train = train_dataset['Label']\n",
        "target_test = test_dataset['Label']"
      ],
      "metadata": {
        "id": "mxUDF0HZIAv_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Distributing the dataset among the clients"
      ],
      "metadata": {
        "id": "lNzDL2-Y3L5N"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "div_list = [np.random.randint(3000,6000) for i in range(NUM_CLIENTS)]\n",
        "origin_list = [np.random.randint(0,125973-6000) for i in range(NUM_CLIENTS)]"
      ],
      "outputs": [],
      "metadata": {
        "id": "ucyWd8g33L5O"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "def prepare_for_training(train_dataset):\n",
        "  target_train = train_dataset['Label']\n",
        "  Y_train = to_categorical(target_train, num_classes=3)\n",
        "  train_dataset = train_dataset.drop(columns = [\"Timestamp\", \"Protocol\",\"PSH Flag Cnt\",\"Init Fwd Win Byts\",\"Flow Byts/s\",\"Flow Pkts/s\", \"Label\"], axis=1)\n",
        "  # making train & test splits\n",
        "  X_train = train_dataset.values\n",
        "  # reshape the data for CNN\n",
        "  X_train = X_train.reshape(len(X_train), X_train.shape[1], 1)\n",
        "  client_train_x = []\n",
        "  client_train_y = []\n",
        "\n",
        "  for i in range(NUM_CLIENTS):\n",
        "    client_train_x.append(X_train[origin_list[i]:origin_list[i]+div_list[i]])\n",
        "    client_train_y.append(Y_train[origin_list[i]:origin_list[i]+div_list[i]])\n",
        "  return X_train,Y_train,client_train_x,client_train_y"
      ],
      "outputs": [],
      "metadata": {
        "id": "16QXZp5Y3L5O"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# # Step 3: Select samples from U using a query function Q,"
      ],
      "metadata": {
        "id": "j6e6ZGKHXf31"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "threshold=len(U_unlabelled_dataset)\n",
        "def new_labelled_dataset(U_unlabelled_dataset,model,step,per=0.4):\n",
        "\n",
        "  X_train = U_unlabelled_dataset.drop(columns = [\"Timestamp\", \"Protocol\",\"PSH Flag Cnt\",\"Init Fwd Win Byts\",\"Flow Byts/s\",\"Flow Pkts/s\", \"Label\"], axis=1)\n",
        "\n",
        "  df =U_unlabelled_dataset.copy()\n",
        "\n",
        "  df[\"score\"]=  model.predict(X_train).max(axis=1)\n",
        "\n",
        "\n",
        "  \n",
        "  df=df.sort_values(by='score', ascending=False)\n",
        "  new_U_labelled_dataset=df.head(int(threshold*per)).drop(columns = [\"score\"], axis=1)\n",
        "\n",
        "  U_unlabelled_dataset = df.tail(threshold-int(threshold*per*step)).drop(columns = [\"score\"], axis=1)\n",
        "  \n",
        "  return U_unlabelled_dataset,new_U_labelled_dataset\n"
      ],
      "metadata": {
        "id": "F5SGRR5zUEKo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 4: request the labels for the samples selected in step 3 from the expert A,\n",
        "\n",
        "#### we don't need this step because we have all the labels"
      ],
      "metadata": {
        "id": "xiEtwOhrXn5Z"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 5: remove the selected samples from the dataset U and add the selected samples to L"
      ],
      "metadata": {
        "id": "1F4ZoyMVXuqt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def update_L_labelled_dataset(new_labelled_dataset):\n",
        "  return pd.concat([train_dataset , new_labelled_dataset])"
      ],
      "metadata": {
        "id": "SZ2bd4b6Xyyb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model Utilities"
      ],
      "metadata": {
        "id": "--KvUPkQ3L5O"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "def create_server_model():\n",
        "    model = Sequential()\n",
        "    model.add(Conv1D(filters=64, kernel_size=6, activation='relu', \n",
        "                    padding='same', input_shape=(73, 1)))\n",
        "    model.add(BatchNormalization())\n",
        "    \n",
        "    # adding a pooling layer\n",
        "    model.add(MaxPooling1D(pool_size=(3), strides=2, padding='same'))\n",
        "    \n",
        "    model.add(Conv1D(filters=64, kernel_size=6, activation='relu', \n",
        "                    padding='same', input_shape=(73, 1)))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(MaxPooling1D(pool_size=(3), strides=2, padding='same'))\n",
        "    \n",
        "    model.add(Conv1D(filters=64, kernel_size=6, activation='relu', \n",
        "                    padding='same', input_shape=(73, 1)))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(MaxPooling1D(pool_size=(3), strides=2, padding='same'))\n",
        "    \n",
        "    model.add(Flatten())\n",
        "    model.add(Dense(64, activation='relu'))\n",
        "    model.add(Dense(64, activation='relu'))\n",
        "    model.add(Dense(3, activation='softmax'))\n",
        "    \n",
        "    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "    return model"
      ],
      "outputs": [],
      "metadata": {
        "id": "cg_6t-U_3L5O"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "def model_cloner(model, learning_rate, optimizer):\n",
        "    new_model = tf.keras.models.clone_model(model)\n",
        "    new_model.set_weights(model.get_weights())\n",
        "    if optimizer=='adam':\n",
        "        new_model.compile(optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate), loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "    return new_model"
      ],
      "outputs": [],
      "metadata": {
        "id": "z2AWs0r-3L5O"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "def train_client(num, model, lr,client_train_x,client_train_y):\n",
        "\n",
        "  new_model = model_cloner(model, lr, 'adam')\n",
        "  hist = new_model.fit(client_train_x[num], client_train_y[num], epochs=1, batch_size=2)\n",
        "\n",
        "  return new_model, lr, round(hist.history['loss'][-1], 4)"
      ],
      "outputs": [],
      "metadata": {
        "id": "ZFCNeyaQ3L5P"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Federated active Learning"
      ],
      "metadata": {
        "id": "MeDP8QFq3L5P"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Step 6: retain the model using the dataset L"
      ],
      "metadata": {
        "id": "VgJv-JeF3L5P"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "client_models = list(np.zeros((NUM_CLIENTS)))\n",
        "arr = list()\n",
        "arr1 = list()\n",
        "server_model_norm = create_server_model()\n",
        "server_model_norm.compile(optimizer = tf.keras.optimizers.Adam(), loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "serverhist1={\n",
        "    \"loss\":[],\n",
        "    \"accuracy\":[],\n",
        "    \"time taken\":[]\n",
        "}\n",
        "step = 0\n",
        "while U_unlabelled_dataset.empty==False:\n",
        "    print( \"Start of step : \" + str(step))\n",
        "    \n",
        "      # Update training data\n",
        "    if step != 0 :\n",
        "      print( \"update unlabled dataset of step : \" + str(step))\n",
        "      U_unlabelled_dataset ,new_labelled_dateset= new_labelled_dataset(U_unlabelled_dataset,server_model_norm,step)\n",
        "      train_dataset = update_L_labelled_dataset(new_labelled_dateset)\n",
        "    else : pass\n",
        "    X_train,Y_train,client_train_x,client_train_y= prepare_for_training(train_dataset)\n",
        "    print( \"U unlabelled dataset: \" + str(len(U_unlabelled_dataset)))\n",
        "\n",
        "    for i in range(NUM_ROUNDS):\n",
        "      print(\"-----\"+str(i)+\"---------\")\n",
        "      losses = []\n",
        "      lr_init = []\n",
        "      data= []\n",
        "      serverhist1[\"time taken\"].append(0)\n",
        "      for j in range(NUM_CLIENTS):\n",
        "        arr.append(time.time())\n",
        "        data.append(train_client(j, server_model_norm, 0.01 ,client_train_x,client_train_y))\n",
        "\n",
        "        client_models[j] = data[j][0]\n",
        "        losses.append(data[j][2])\n",
        "        lr_init.append(data[j][1])\n",
        "        # time.sleep(2.519+random.random()*3-1.5)\n",
        "        arr[-1] = time.time()-arr[-1]+2.519+random.random()*3-1.5\n",
        "        serverhist1[\"time taken\"][-1] = max(serverhist1[\"time taken\"][-1], arr[-1])\n",
        "\n",
        "      print(arr)\n",
        "      # Aggregating model\n",
        "      arr1.append(max(arr))\n",
        "      sum1=[i*0 for i in client_models[0].get_weights()]\n",
        "      for i in range(NUM_CLIENTS):\n",
        "        sum1 = [i+j for i, j in zip(client_models[i].get_weights(), sum1)]\n",
        "      server_model_norm.set_weights([i/NUM_CLIENTS for i in sum1])\n",
        "      h=server_model_norm.evaluate(X_train[:5000, :],Y_train[:5000, :])\n",
        "      serverhist1['loss'].append(h[0])\n",
        "      serverhist1['accuracy'].append(h[1])\n",
        "    print( \"End of step : \" + str(step))\n",
        "    step+=1\n",
        "\n",
        "\n",
        "print( \"End of training\")"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Start of step : 0\n",
            "U unlabelled dataset: 837039\n",
            "-----0---------\n",
            "2562/2562 [==============================] - 15s 5ms/step - loss: 0.5737 - accuracy: 0.7826\n",
            "2801/2801 [==============================] - 16s 5ms/step - loss: 0.5337 - accuracy: 0.7934\n",
            "2347/2347 [==============================] - 13s 5ms/step - loss: 0.6027 - accuracy: 0.7746\n",
            "2008/2008 [==============================] - 12s 5ms/step - loss: 0.5940 - accuracy: 0.7848\n",
            " 445/2744 [===>..........................] - ETA: 12s - loss: 0.8985 - accuracy: 0.7045"
          ]
        }
      ],
      "metadata": {
        "tags": [
          "outputPrepend"
        ],
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_kEJLJro3L5P",
        "outputId": "0b465415-4094-4331-f1a8-b49b168cb435"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Saving Training Metrics"
      ],
      "metadata": {
        "id": "_cz4g9BB3L5Q"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "with open(\"/content/normal_out.json\", \"w\") as f:\n",
        "    dump(serverhist1, f, indent=4)"
      ],
      "outputs": [],
      "metadata": {
        "id": "x1dV4Am_3L5Q"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Loading Training Metrics"
      ],
      "metadata": {
        "id": "GWOIVFgw3L5R"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "with open(\"/content/normal_out.json\", \"r\") as f:\n",
        "    serverhist1 = load(f)"
      ],
      "outputs": [],
      "metadata": {
        "id": "MtnZBRqU3L5R"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Time Taken per Round"
      ],
      "metadata": {
        "id": "Dr9j7WtW3L5R"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "plt.plot(serverhist1[\"time taken\"])\n",
        "plt.ylabel(\"Time Taken\")\n",
        "plt.xlabel(\"Round Number\")\n",
        "plt.title(\"Time Taken per Round\")\n",
        "plt.grid()\n",
        "\n",
        "plt.show()\n",
        "plt.show()"
      ],
      "outputs": [],
      "metadata": {
        "id": "PuDIbbos3L5R"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Performance of the server model after each round"
      ],
      "metadata": {
        "id": "fperQwoA3L5R"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "fig = plt.figure(dpi=300, figsize=(12, 6))\n",
        "ax = fig.add_subplot(121)\n",
        "ax.plot(serverhist1[\"accuracy\"], color='green')\n",
        "ax.grid()\n",
        "ax.set_xlabel(\"Rounds\")\n",
        "ax.xaxis.set_minor_locator(MultipleLocator(1))\n",
        "ax.set_ylabel(\"Accuracy\")\n",
        "ax.set_title(\"Accuracy\")\n",
        "ax = fig.add_subplot(122)\n",
        "ax.plot(serverhist1[\"loss\"], color='#FF4800')\n",
        "ax.grid()\n",
        "ax.set_xlabel(\"Rounds\")\n",
        "ax.xaxis.set_minor_locator(MultipleLocator(1))\n",
        "ax.set_ylabel(\"Loss\")\n",
        "ax.set_title(\"Loss\")\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "outputs": [],
      "metadata": {
        "id": "PIXbeqje3L5S"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "conf_matrix = confusion_matrix(np.argmax(Y_train, axis=1), np.argmax(server_model_norm.predict(X_train), axis=1)).numpy()\n",
        "fig, ax = plt.subplots(figsize=(7.5, 7.5))\n",
        "ax.matshow(conf_matrix, cmap=\"Greens\", alpha=0.6, vmin=-1000)\n",
        "for i in range(conf_matrix.shape[0]):\n",
        "    for j in range(conf_matrix.shape[1]):\n",
        "        ax.text(x=j, y=i,s=conf_matrix[i, j], va='center', ha='center', size='xx-large')\n",
        "ax.set_xticks(np.arange(3))\n",
        "ax.set_yticks(np.arange(3))\n",
        "ax.set_xticklabels([\"Benign\",\"bruteforce\",\"bruteforceSSH\"], size='x-large')\n",
        "ax.set_yticklabels([\"Benign\",\"bruteforce\",\"bruteforceSSH\"], rotation=90, va=\"center\", size='x-large')\n",
        "plt.ylabel('Actuals', fontsize=18)\n",
        "plt.title('Predictions', fontsize=18)\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "outputs": [],
      "metadata": {
        "id": "8l18W4-23L5V"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "gQ9hPMlhX6Nk"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3.8.10 64-bit"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.10"
    },
    "metadata": {
      "interpreter": {
        "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
      }
    },
    "orig_nbformat": 2,
    "interpreter": {
      "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}